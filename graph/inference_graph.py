from langgraph.graph import StateGraph, START, END
import asyncio

from config.getenv import GetEnv
from core.state import State
from node.nodes import generator_node, evaluator_node, reflector_node, curator_node, retriever_playbook_node, update_playbook_node
from graph.graph_utils import solution_stream

env = GetEnv()


def create_inference_graph():
    builder = StateGraph(State)

    builder.add_node("retriever", retriever_playbook_node)
    builder.add_node("generator", generator_node)
    builder.add_node("evaluator", evaluator_node)
    builder.add_node("reflector", reflector_node)
    builder.add_node("curator", curator_node)
    builder.add_node("update", update_playbook_node)

    builder.add_edge(START, "retriever")
    builder.add_edge("retriever", "generator")
    builder.add_edge("generator", "evaluator")
    builder.add_edge("evaluator", "reflector")
    builder.add_edge("reflector", "curator")
    builder.add_edge("curator", "update")
    builder.add_edge("update", END)

    return builder.compile()

async def run_query(inference_graph, state, query: str):
    state["query"] = query

    async for token in solution_stream(inference_graph, state):
        print(token, end="", flush=True)

    print("\n")
    return state


async def main():
    inference_graph = create_inference_graph()

    # ì´ˆê¸° state
    state = {
        "query": "",
        "playbook": [],   # âœ… í•œ ë²ˆë§Œ ì´ˆê¸°í™”í•˜ê³  ì´í›„ ê³„ì† ê°±ì‹ ë¨
        "solution": "",
        "verbose": True,
        "retrieved_bullets": [],
        "used_bullet_ids": [],
        "trajectory": [],
        "reflection": {},
        "new_insights": [],
        "feedback": {},
        "current_step": 0,
        "max_playbook_size": env.get_playbook_config["MAX_PLAYBOOK_SIZE"],
        "dedup_threshold": env.get_playbook_config["DEDUP_THRESHOLD"],
        "retrieval_threshold": env.get_playbook_config["RETRIEVAL_THRESHOLD"],
    }

    # âœ… ì—¬ëŸ¬ ë²ˆì˜ ì§ˆë¬¸ ì‹¤í–‰ (í•™ìŠµ ëˆ„ì )
    # âœ… ADD, UPDATE, DEDUP, PRUNEì„ í…ŒìŠ¤íŠ¸í•˜ê¸° ìœ„í•œ ì§ˆë¬¸ ì‹œë‚˜ë¦¬ì˜¤
    queries = [
        # 1. ADD: .sort()ì— ëŒ€í•œ ìƒˆë¡œìš´ ì •ë³´ ì¶”ê°€
        "íŒŒì´ì¬ì—ì„œ ë¦¬ìŠ¤íŠ¸ë¥¼ ì •ë ¬í•˜ëŠ” ê°€ì¥ ê¸°ë³¸ì ì¸ ë°©ë²•ì€ ë­ì•¼?",
        # 2. ADD: sorted()ì— ëŒ€í•œ ìƒˆë¡œìš´ ì •ë³´ ì¶”ê°€
        "íŒŒì´ì¬ì—ì„œ ì›ë˜ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°”ê¾¸ì§€ ì•Šê³  ìƒˆë¡­ê²Œ ì •ë ¬ëœ ë¦¬ìŠ¤íŠ¸ë¥¼ ë§Œë“¤ë ¤ë©´ ì–´ë–»ê²Œ í•´?",
        # 3. UPDATE: .sort()ì— reverse=True íŒŒë¼ë¯¸í„°ë¥¼ ì¶”ê°€í•˜ë„ë¡ ì—…ë°ì´íŠ¸ ìœ ë„
        "íŒŒì´ì¬ ë¦¬ìŠ¤íŠ¸ë¥¼ ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ ì •ë ¬í•˜ë ¤ë©´ ì–´ë–»ê²Œ í•´ì•¼ í•´?",
        # 4. DEDUP: 1ë²ˆ ì§ˆë¬¸ê³¼ ìœ ì‚¬í•œ ì§ˆë¬¸ìœ¼ë¡œ ì¤‘ë³µ ì¶”ê°€ ë°©ì§€ í…ŒìŠ¤íŠ¸
        "íŒŒì´ì¬ ë¦¬ìŠ¤íŠ¸ë¥¼ ì •ë ¬í•˜ëŠ” ë°©ë²• ì•Œë ¤ì¤˜.",
        # 5. PRUNE: ì˜ëª»ëœ ì •ë³´(ìˆ«ì/ë¬¸ì í˜¼í•© ì •ë ¬)ë¥¼ ì œê³µí•˜ì—¬ harmful_countë¥¼ ë†’ì´ê³ , í•´ë‹¹ í•­ëª© ì‚­ì œ ìœ ë„
        "íŒŒì´ì¬ì—ì„œ ìˆ«ìì™€ ë¬¸ìê°€ ì„ì¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ì •ë ¬í•  ìˆ˜ ìˆì–´?",
    ]

    for i, q in enumerate(queries, start=1):
        print(f"\n==== ğŸ§  STEP {i}: {q} ====\n")
        state["current_step"] = i
        state = await run_query(inference_graph, state, q)

if __name__ == "__main__":
    asyncio.run(main())