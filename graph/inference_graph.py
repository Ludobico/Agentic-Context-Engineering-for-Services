from langgraph.graph import StateGraph, START, END
import asyncio

from config.getenv import GetEnv
from core.state import State
from node.nodes import generator_node, evaluator_node, reflector_node, curator_node, retriever_playbook_node, update_playbook_node
from graph.graph_utils import solution_stream

env = GetEnv()


def create_inference_graph():
    builder = StateGraph(State)

    builder.add_node("retriever", retriever_playbook_node)
    builder.add_node("generator", generator_node)
    builder.add_node("evaluator", evaluator_node)
    builder.add_node("reflector", reflector_node)
    builder.add_node("curator", curator_node)
    builder.add_node("update", update_playbook_node)

    builder.add_edge(START, "retriever")
    builder.add_edge("retriever", "generator")
    builder.add_edge("generator", "evaluator")
    builder.add_edge("evaluator", "reflector")
    builder.add_edge("reflector", "curator")
    builder.add_edge("curator", "update")
    builder.add_edge("update", END)

    return builder.compile()

async def run_query(inference_graph, state, query: str):
    state["query"] = query

    async for token in solution_stream(inference_graph, state):
        print(token, end="", flush=True)

    print("\n")
    return state


async def main():
    inference_graph = create_inference_graph()

    # ì´ˆê¸° state
    state = {
        "query": "",
        "playbook": [],   # âœ… í•œ ë²ˆë§Œ ì´ˆê¸°í™”í•˜ê³  ì´í›„ ê³„ì† ê°±ì‹ ë¨
        "solution": "",
        "verbose": True,
        "retrieved_bullets": [],
        "used_bullet_ids": [],
        "trajectory": [],
        "reflection": {},
        "new_insights": [],
        "feedback": {},
        "current_step": 0,
        "max_playbook_size": env.get_playbook_config["MAX_PLAYBOOK_SIZE"],
        "dedup_threshold": env.get_playbook_config["DEDUP_THRESHOLD"],
        "retrieval_threshold": env.get_playbook_config["RETRIEVAL_THRESHOLD"],
    }

    # âœ… ì—¬ëŸ¬ ë²ˆì˜ ì§ˆë¬¸ ì‹¤í–‰ (í•™ìŠµ ëˆ„ì )
    queries = [
        "ë…ìˆ˜ë¦¬ ë¶€ë¦¬ëŠ” ì™œ ë…¸ë„ê¹Œ?",
        "ë¶€ì—‰ì´ì™€ ë…ìˆ˜ë¦¬ì˜ ì‹œë ¥ ì°¨ì´ëŠ” ë­˜ê¹Œ?",
        "ë…ìˆ˜ë¦¬ëŠ” ì‚¬ëƒ¥ì„ ì–´ë–»ê²Œ ê³„íší• ê¹Œ?",
    ]

    for i, q in enumerate(queries, start=1):
        print(f"\n==== ğŸ§  STEP {i}: {q} ====\n")
        state["current_step"] = i
        state = await run_query(inference_graph, state, q)

if __name__ == "__main__":
    asyncio.run(main())